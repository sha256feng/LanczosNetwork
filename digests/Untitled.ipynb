{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze \"mpnn.py\" script\n",
    "  MPNN is a fairly good neural network framework on graph NNs. Here we analyze the script for MPNN implementation in PyTorch, its argument inputs, the algorithms etc. \n",
    "  \n",
    "This 'mpnn.py' script is located here: LanczosNetwork/model/mpnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archietecture of MPNN\n",
    "1. Initialize parameters;  \n",
    "2. Forward;  \n",
    "3. Message propagate;  \n",
    "4. Output;  \n",
    "5. Evaluation metrics;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model.set2set import Set2Vec \n",
    "# Set2Vec, Set2Set: iterative content-based attention\n",
    "from operators.functions.unsorted_segment_sum import UnsortedSegmentSumFunction\n",
    "\n",
    "# eps: smallest representatble number\n",
    "EPS = float(np.finfo(np.float32).eps)\n",
    "unsorted_segment_sum = UnsortedSegmentSumFunction.apply\n",
    "\n",
    "__all__ = ['MPNN']\n",
    "\n",
    "\n",
    "class MPNN(nn.Module):\n",
    "\n",
    "  def __init__(self, config):\n",
    "    \"\"\" Message Passing Neural Networks,\n",
    "        see reference below for more information\n",
    "        Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O. and Dahl,\n",
    "        G.E., 2017. Neural message passing for quantum chemistry. In ICML.\n",
    "    \"\"\"\n",
    "    super(MPNN, self).__init__()\n",
    "    \n",
    "    # config is in a yaml file, dictating the parameters for mpnn;\n",
    "    self.config = config\n",
    "    self.input_dim = config.model.input_dim\n",
    "    self.hidden_dim = config.model.hidden_dim\n",
    "    self.output_dim = config.model.output_dim\n",
    "    self.num_layer = config.model.num_layer\n",
    "    self.num_prop = config.model.num_prop\n",
    "    self.msg_func_name = config.model.msg_func\n",
    "    self.num_step_set2vec = config.model.num_step_set2vec\n",
    "    self.dropout = config.model.dropout if hasattr(config.model,\n",
    "                                                   'dropout') else 0.0\n",
    "    self.num_atom = config.dataset.num_atom\n",
    "    self.num_edgetype = config.dataset.num_bond_type\n",
    "    self.aggregate_type = config.model.aggregate_type\n",
    "    assert self.num_layer == 1, 'not implemented'\n",
    "    assert self.aggregate_type in ['avg', 'sum'], 'not implemented'\n",
    "\n",
    "    self.node_embedding = nn.Embedding(self.num_atom, self.input_dim)\n",
    "\n",
    "    # input function\n",
    "    self.input_func = nn.Sequential(\n",
    "        *[nn.Linear(self.input_dim, self.hidden_dim)])\n",
    "\n",
    "    # update function\n",
    "    self.update_func = nn.GRUCell(\n",
    "        input_size=self.hidden_dim * (self.num_edgetype + 1),\n",
    "        hidden_size=self.hidden_dim)\n",
    "\n",
    "    # message function\n",
    "    # N.B.: if there is no edge feature, the edge network in the paper degenerates\n",
    "    # to multiple edge embedding matrices, each corresponds to one edge type\n",
    "    if config.model.msg_func == 'embedding':\n",
    "      self.edge_embedding = nn.Embedding(self.num_edgetype + 1, self.hidden_dim\n",
    "                                         **2)\n",
    "    elif config.model.msg_func == 'MLP':\n",
    "      self.edge_func = nn.ModuleList([\n",
    "          nn.Sequential(*[\n",
    "              nn.Linear(self.hidden_dim * 2, 64),\n",
    "              nn.ReLU(),\n",
    "              nn.Linear(64, self.hidden_dim)\n",
    "          ]) for _ in range((self.num_edgetype + 1))\n",
    "      ])\n",
    "    else:\n",
    "      raise ValueError('Non-supported message function')\n",
    "\n",
    "    self.att_func = Set2Vec(self.hidden_dim, self.num_step_set2vec)\n",
    "\n",
    "    # output function\n",
    "    self.output_func = nn.Sequential(\n",
    "        *[nn.Linear(2 * self.hidden_dim, self.output_dim)])\n",
    "\n",
    "    if config.model.loss == 'CrossEntropy':\n",
    "      self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "    elif config.model.loss == 'MSE':\n",
    "      self.loss_func = torch.nn.MSELoss()\n",
    "    elif config.model.loss == 'L1':\n",
    "      self.loss_func = torch.nn.L1Loss()\n",
    "    else:\n",
    "      raise ValueError(\"Non-supported loss function!\")\n",
    "\n",
    "    self._init_param()\n",
    "\n",
    "  def _init_param(self):\n",
    "    mlp_modules = [\n",
    "        xx for xx in [self.input_func, self.output_func, self.att_func]\n",
    "        if xx is not None\n",
    "    ]\n",
    "\n",
    "    for m in mlp_modules:\n",
    "      if isinstance(m, nn.Sequential):\n",
    "        for mm in m:\n",
    "          if isinstance(mm, nn.Linear):\n",
    "            nn.init.xavier_uniform_(mm.weight.data)\n",
    "            if mm.bias is not None:\n",
    "              mm.bias.data.zero_()\n",
    "      elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        if m.bias is not None:\n",
    "          m.bias.data.zero_()\n",
    "\n",
    "    for m in [self.update_func]:\n",
    "      nn.init.xavier_uniform_(m.weight_hh.data)\n",
    "      nn.init.xavier_uniform_(m.weight_ih.data)\n",
    "      if m.bias:\n",
    "        m.bias_hh.data.zero_()\n",
    "        m.bias_ih.data.zero_()\n",
    "\n",
    "  def forward(self, node_feat, L, label=None, mask=None):\n",
    "    \"\"\"\n",
    "      shape parameters:\n",
    "        batch size = B\n",
    "        embedding dim = D\n",
    "        max number of nodes within one mini batch = N\n",
    "        number of edge types = E\n",
    "        number of predicted properties = P\n",
    "      \n",
    "      Args:\n",
    "        node_feat: long tensor, shape B X N\n",
    "        L: float tensor, shape B X N X N X (E + 1)\n",
    "        label: float tensor, shape B X P\n",
    "        mask: float tensor, shape B X N\n",
    "    \"\"\"    \n",
    "    L[L != 0] = 1.0\n",
    "    batch_size = node_feat.shape[0]\n",
    "    num_node = node_feat.shape[1]\n",
    "    state = self.node_embedding(node_feat)  # shape: B X N X D\n",
    "    state = self.input_func(state)\n",
    "\n",
    "    if self.msg_func_name == 'MLP':\n",
    "      idx_row, idx_col = np.meshgrid(range(num_node), range(num_node))\n",
    "      idx_row, idx_col = idx_row.flatten().astype(\n",
    "          np.int64), idx_col.flatten().astype(np.int64)\n",
    "\n",
    "    def _prop(state_old):\n",
    "      state_dim = state_old.shape[2]\n",
    "\n",
    "      msg = []\n",
    "      for ii in range(self.num_edgetype + 1):\n",
    "        if self.msg_func_name == 'embedding':\n",
    "          idx_edgetype = torch.Tensor([ii]).long().to(node_feat.device)\n",
    "          edge_em = self.edge_embedding(idx_edgetype).view(state_dim, state_dim)\n",
    "          node_state = state_old.view(batch_size * num_node,\n",
    "                                      -1)  # shape: BN X D\n",
    "          tmp_msg = node_state.mm(edge_em).view(batch_size, num_node,\n",
    "                                                -1)  # shape: B X N X D\n",
    "          # aggregate message\n",
    "          if self.aggregate_type == 'sum':\n",
    "            tmp_msg = torch.bmm(L[:, :, :, ii], tmp_msg)\n",
    "          elif self.aggregate_type == 'avg':\n",
    "            denom = torch.sum(L[:, :, :, ii], dim=2, keepdim=True) + EPS\n",
    "            tmp_msg = torch.bmm(L[:, :, :, ii] / denom, tmp_msg)\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        elif self.msg_func_name == 'MLP':\n",
    "          state_in = state_old[:, idx_col, :]  # shape: B X N X D\n",
    "          state_out = state_old[:, idx_row, :]  # shape: B X N X D\n",
    "          tmp_msg = self.edge_func[ii](torch.cat(\n",
    "              [state_out, state_in], dim=2).view(\n",
    "                  batch_size * num_node * num_node, -1)).view(\n",
    "                      batch_size, num_node, num_node,\n",
    "                      -1)  # shape: B X N X N X D\n",
    "\n",
    "          # aggregate message\n",
    "          if self.aggregate_type == 'sum':\n",
    "            tmp_msg = torch.matmul(\n",
    "                tmp_msg.permute(0, 1, 3, 2),\n",
    "                L[:, :, :, ii].unsqueeze(dim=3)).squeeze()  # B X N X D\n",
    "          elif self.aggregate_type == 'avg':\n",
    "            denom = torch.sum(\n",
    "                L[:, :, :, ii], dim=2, keepdim=True) + EPS  # B X N X 1\n",
    "            tmp_msg = torch.matmul(\n",
    "                tmp_msg.permute(0, 1, 3, 2),\n",
    "                L[:, :, :, ii].unsqueeze(dim=3)).squeeze()  # B X N X D\n",
    "            tmp_msg = tmp_msg / denom\n",
    "          else:\n",
    "            pass\n",
    "\n",
    "        msg += [tmp_msg]  # shape B X N X D\n",
    "\n",
    "      # update state\n",
    "      msg = torch.cat(msg, dim=2).view(batch_size * num_node, -1)\n",
    "      state_old = state_old.view(batch_size * num_node, -1)\n",
    "\n",
    "      # GRU update\n",
    "      state_new = self.update_func(msg, state_old).view(batch_size, num_node,\n",
    "                                                        -1)\n",
    "\n",
    "      return state_new\n",
    "\n",
    "    # propagation\n",
    "    for tt in range(self.num_prop):\n",
    "      state = _prop(state)\n",
    "      state = F.dropout(state, self.dropout, training=self.training)\n",
    "\n",
    "    # output\n",
    "    y = []\n",
    "    if mask is not None:\n",
    "      for bb in range(batch_size):\n",
    "        y += [self.att_func(state[bb, mask[bb], :])]\n",
    "    else:\n",
    "      for bb in range(batch_size):\n",
    "        y += [self.att_func(state[bb, :, :])]\n",
    "\n",
    "    score = self.output_func(torch.cat(y, dim=0))\n",
    "\n",
    "    if label is not None:\n",
    "      return score, self.loss_func(score, label)\n",
    "    else:\n",
    "      return score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
